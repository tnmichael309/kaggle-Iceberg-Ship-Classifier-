{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_257</th>\n",
       "      <th>f_258</th>\n",
       "      <th>f_259</th>\n",
       "      <th>f_260</th>\n",
       "      <th>f_261</th>\n",
       "      <th>f_262</th>\n",
       "      <th>f_263</th>\n",
       "      <th>f_264</th>\n",
       "      <th>f_265</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.078176</td>\n",
       "      <td>0.358596</td>\n",
       "      <td>-0.215716</td>\n",
       "      <td>-0.372415</td>\n",
       "      <td>-1.290822</td>\n",
       "      <td>0.039064</td>\n",
       "      <td>0.134564</td>\n",
       "      <td>0.787661</td>\n",
       "      <td>-0.613845</td>\n",
       "      <td>0.108084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268202</td>\n",
       "      <td>-0.910513</td>\n",
       "      <td>0.280417</td>\n",
       "      <td>0.250057</td>\n",
       "      <td>-0.042847</td>\n",
       "      <td>0.724692</td>\n",
       "      <td>-0.741978</td>\n",
       "      <td>-0.324261</td>\n",
       "      <td>0.286298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.273724</td>\n",
       "      <td>1.087702</td>\n",
       "      <td>-1.793496</td>\n",
       "      <td>-1.264303</td>\n",
       "      <td>0.042096</td>\n",
       "      <td>0.182288</td>\n",
       "      <td>0.078520</td>\n",
       "      <td>0.412620</td>\n",
       "      <td>0.980071</td>\n",
       "      <td>-1.091136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.753098</td>\n",
       "      <td>-0.415596</td>\n",
       "      <td>-0.035685</td>\n",
       "      <td>-1.136880</td>\n",
       "      <td>-0.160498</td>\n",
       "      <td>-0.886964</td>\n",
       "      <td>-0.399335</td>\n",
       "      <td>-2.055109</td>\n",
       "      <td>0.922215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.481582</td>\n",
       "      <td>-1.725730</td>\n",
       "      <td>0.391214</td>\n",
       "      <td>-1.857765</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>-0.145043</td>\n",
       "      <td>-0.434842</td>\n",
       "      <td>-0.503897</td>\n",
       "      <td>-0.264354</td>\n",
       "      <td>0.272160</td>\n",
       "      <td>...</td>\n",
       "      <td>1.063322</td>\n",
       "      <td>-0.392458</td>\n",
       "      <td>1.106920</td>\n",
       "      <td>-0.279701</td>\n",
       "      <td>1.667911</td>\n",
       "      <td>1.040562</td>\n",
       "      <td>0.447926</td>\n",
       "      <td>-1.486754</td>\n",
       "      <td>-0.780378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.089239</td>\n",
       "      <td>0.445182</td>\n",
       "      <td>0.128932</td>\n",
       "      <td>0.059036</td>\n",
       "      <td>-0.324501</td>\n",
       "      <td>0.164366</td>\n",
       "      <td>0.122997</td>\n",
       "      <td>1.574274</td>\n",
       "      <td>-0.467762</td>\n",
       "      <td>0.552849</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085721</td>\n",
       "      <td>-1.308412</td>\n",
       "      <td>-0.889094</td>\n",
       "      <td>1.183155</td>\n",
       "      <td>-1.483414</td>\n",
       "      <td>0.219054</td>\n",
       "      <td>-0.598839</td>\n",
       "      <td>-0.953547</td>\n",
       "      <td>-0.252878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.038766</td>\n",
       "      <td>0.221048</td>\n",
       "      <td>-0.418525</td>\n",
       "      <td>0.133892</td>\n",
       "      <td>-0.974831</td>\n",
       "      <td>-0.889494</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>1.753731</td>\n",
       "      <td>-0.450525</td>\n",
       "      <td>0.435784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541727</td>\n",
       "      <td>0.168227</td>\n",
       "      <td>0.190182</td>\n",
       "      <td>1.246043</td>\n",
       "      <td>0.217847</td>\n",
       "      <td>0.486931</td>\n",
       "      <td>-0.417607</td>\n",
       "      <td>0.094914</td>\n",
       "      <td>-1.127213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 267 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_0       f_1       f_2       f_3       f_4       f_5       f_6  \\\n",
       "0 -1.078176  0.358596 -0.215716 -0.372415 -1.290822  0.039064  0.134564   \n",
       "1 -1.273724  1.087702 -1.793496 -1.264303  0.042096  0.182288  0.078520   \n",
       "2  0.481582 -1.725730  0.391214 -1.857765  0.999916 -0.145043 -0.434842   \n",
       "3 -1.089239  0.445182  0.128932  0.059036 -0.324501  0.164366  0.122997   \n",
       "4 -1.038766  0.221048 -0.418525  0.133892 -0.974831 -0.889494  0.028311   \n",
       "\n",
       "        f_7       f_8       f_9     ...         f_257     f_258     f_259  \\\n",
       "0  0.787661 -0.613845  0.108084     ...      0.268202 -0.910513  0.280417   \n",
       "1  0.412620  0.980071 -1.091136     ...     -0.753098 -0.415596 -0.035685   \n",
       "2 -0.503897 -0.264354  0.272160     ...      1.063322 -0.392458  1.106920   \n",
       "3  1.574274 -0.467762  0.552849     ...      1.085721 -1.308412 -0.889094   \n",
       "4  1.753731 -0.450525  0.435784     ...     -0.541727  0.168227  0.190182   \n",
       "\n",
       "      f_260     f_261     f_262     f_263     f_264     f_265  is_iceberg  \n",
       "0  0.250057 -0.042847  0.724692 -0.741978 -0.324261  0.286298           0  \n",
       "1 -1.136880 -0.160498 -0.886964 -0.399335 -2.055109  0.922215           0  \n",
       "2 -0.279701  1.667911  1.040562  0.447926 -1.486754 -0.780378           1  \n",
       "3  1.183155 -1.483414  0.219054 -0.598839 -0.953547 -0.252878           0  \n",
       "4  1.246043  0.217847  0.486931 -0.417607  0.094914 -1.127213           0  \n",
       "\n",
       "[5 rows x 267 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('Data/pca_projected_266_from_resnet_train.csv')\n",
    "#train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 266) (1604,)\n"
     ]
    }
   ],
   "source": [
    "features = train_df.columns.tolist()\n",
    "features.remove('is_iceberg')\n",
    "\n",
    "train_X = train_df[features]\n",
    "train_y = np.array(train_df['is_iceberg']).reshape((train_X.shape[0],))\n",
    "\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_iter': 10000, 'penalty': 'l2', 'random_state': 4, 'solver': 'saga'} -0.304834935092\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "params = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'penalty': ['l2'],\n",
    "    'random_state': [1,2,3,4,5],\n",
    "    'max_iter': [10000]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lr, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_iter': 10000, 'penalty': 'l1', 'random_state': 1, 'solver': 'saga'} -0.248148829504\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "params = {\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'penalty': ['l1'],\n",
    "    'random_state': [1,2,3,4,5],\n",
    "    'max_iter': [10000]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lr, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'sgd'} -0.223207087883\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(learning_rate='adaptive', \n",
    "                    learning_rate_init=0.005, \n",
    "                    max_iter=2000, \n",
    "                    random_state=0)\n",
    "params = {\n",
    "    'solver':['lbfgs', 'sgd', 'adam']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(mlp, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (150,)} -0.212279414994\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(learning_rate='adaptive', \n",
    "                    learning_rate_init=0.005, \n",
    "                    max_iter=2000, \n",
    "                    random_state=0, \n",
    "                    solver='sgd')\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(100,), (150,), (100, 100,)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(mlp, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 5.0} -0.172329891338\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(learning_rate='adaptive', \n",
    "                    learning_rate_init=0.005, \n",
    "                    max_iter=2000, \n",
    "                    random_state=0, \n",
    "                    solver='sgd',\n",
    "                    hidden_layer_sizes=(150,))\n",
    "params = {\n",
    "    'alpha': [10., 5., 2., 1., .8, .5, .2, .1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(mlp, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_state': 0} -0.172329891338\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(learning_rate='adaptive', \n",
    "                    learning_rate_init=0.005, \n",
    "                    max_iter=2000, \n",
    "                    random_state=0, \n",
    "                    solver='sgd',\n",
    "                    hidden_layer_sizes=(150,),\n",
    "                    alpha=5.,)\n",
    "params = {\n",
    "    'random_state': [0, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(mlp, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'n_estimators': 100} -0.345648843517\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=200, learning_rate=1.0, algorithm='SAMME.R', random_state=0)\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'learning_rate': [1.0, 0.5, 0.1, 0.05]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(ada, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_features': 'sqrt'} -0.306649477945\n",
      "{'bootstrap': False, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': 1, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=100, criterion='entropy', random_state=0)\n",
    "\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(et, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'max_leaf_nodes': 10} -0.146591943787\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': 10, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 400, 'presort': 'auto', 'random_state': 0, 'subsample': 1.0, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=400, max_features='sqrt', learning_rate=0.05, random_state=0)\n",
    "params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'max_leaf_nodes': [5, 10, 20]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None} -0.273771535346\n",
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_jobs': 1, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, criterion='entropy', max_features='sqrt', random_state=0)\n",
    "params = {\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    #'max_leaf_nodes': [5, 10, 20]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(train_X, train_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=10000, penalty='l1', random_state= 1, solver='saga')\n",
    "mlp = MLPClassifier(learning_rate='adaptive', \n",
    "                    learning_rate_init=0.005, \n",
    "                    max_iter=2000, \n",
    "                    random_state=0, \n",
    "                    solver='sgd',\n",
    "                    hidden_layer_sizes=(150,),\n",
    "                    alpha=5.,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training stacking averaged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stacking_models_api import StackingAveragedModels\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sl_base_models_dict = {\n",
    "    'lr_1': LogisticRegression(max_iter=2000, penalty='l1', random_state= 1, solver='saga'),\n",
    "    'lr_2': LogisticRegression(max_iter=2000, penalty='l1', random_state= 1, solver='liblinear'),\n",
    "    'mlp_1': MLPClassifier(learning_rate='adaptive', \n",
    "                    learning_rate_init=0.005, \n",
    "                    max_iter=2000, \n",
    "                    random_state=0, \n",
    "                    solver='sgd',\n",
    "                    hidden_layer_sizes=(150,),\n",
    "                    alpha=5.,),\n",
    "    'mlp_2': MLPClassifier(learning_rate='adaptive', \n",
    "                    learning_rate_init=0.005, \n",
    "                    max_iter=2000, \n",
    "                    random_state=0, \n",
    "                    solver='adam',\n",
    "                    hidden_layer_sizes=(150,),\n",
    "                    alpha=5.,),\n",
    "    \n",
    "    'adb_1': AdaBoostClassifier(n_estimators=100, learning_rate=.01),\n",
    "    'adb_2': AdaBoostClassifier(n_estimators=100, learning_rate=.005),\n",
    "    'adb_3': AdaBoostClassifier(n_estimators=100, learning_rate=.001),\n",
    "    'adb_4': AdaBoostClassifier(n_estimators=400, learning_rate=.001),\n",
    "    'adb_5': AdaBoostClassifier(n_estimators=400, learning_rate=.0005),\n",
    "    'adb_6': AdaBoostClassifier(n_estimators=400, learning_rate=.0001),\n",
    "    \n",
    "    'bg_1': BaggingClassifier(n_estimators=200, max_features=.8, max_samples=.8, random_state=0),\n",
    "    'bg_2': BaggingClassifier(n_estimators=200, max_features=.7, max_samples=.7, random_state=0),\n",
    "    'bg_3': BaggingClassifier(n_estimators=200, max_features=.9, max_samples=.9, random_state=0),\n",
    "    'bg_4': BaggingClassifier(n_estimators=200, max_features=.8, max_samples=.8, bootstrap_features=True, random_state=0),\n",
    "    'bg_5': BaggingClassifier(n_estimators=200, max_features=.7, max_samples=.7, bootstrap_features=True, random_state=0),\n",
    "    'bg_6': BaggingClassifier(n_estimators=200, max_features=.6, max_samples=.6, bootstrap_features=True, random_state=0),\n",
    "    \n",
    "    'gb_1': GradientBoostingClassifier(n_estimators=200, max_features='sqrt', learning_rate=0.05, \n",
    "                                       max_depth=5, max_leaf_nodes=10, random_state=0),\n",
    "    'gb_2': GradientBoostingClassifier(n_estimators=400, max_features='sqrt', learning_rate=0.05, \n",
    "                                       max_depth=3, max_leaf_nodes=5, random_state=0),\n",
    "    'gb_3': GradientBoostingClassifier(n_estimators=800, max_features='sqrt', learning_rate=0.05, \n",
    "                                       max_depth=2, max_leaf_nodes=3, random_state=0),\n",
    "    'gb_4': GradientBoostingClassifier(n_estimators=200, max_features='sqrt', learning_rate=0.05, \n",
    "                                       max_depth=5, max_leaf_nodes=10, subsample=.8, random_state=0),\n",
    "    'gb_5': GradientBoostingClassifier(n_estimators=400, max_features='sqrt', learning_rate=0.05, \n",
    "                                       max_depth=3, max_leaf_nodes=5, subsample=.8, random_state=0),\n",
    "    'gb_6': GradientBoostingClassifier(n_estimators=800, max_features='sqrt', learning_rate=0.05, \n",
    "                                       max_depth=2, max_leaf_nodes=3, subsample=.8, random_state=0),\n",
    "    \n",
    "    'rf_1': RandomForestClassifier(n_estimators=200, criterion='entropy', max_features='sqrt', random_state=0),\n",
    "    'rf_2': RandomForestClassifier(n_estimators=200, criterion='entropy', max_features='sqrt', max_depth=4, random_state=0),\n",
    "    'rf_3': RandomForestClassifier(n_estimators=200, criterion='entropy', max_features='sqrt', max_depth=8, random_state=0),\n",
    "    \n",
    "    'et_1': ExtraTreesClassifier(n_estimators=100, criterion='entropy', max_features='sqrt', random_state=0),\n",
    "    'et_2': ExtraTreesClassifier(n_estimators=100, criterion='entropy', max_features='sqrt', max_depth=4, random_state=0),\n",
    "    'et_3': ExtraTreesClassifier(n_estimators=100, criterion='entropy', max_features='sqrt', max_depth=8, random_state=0),\n",
    "    'et_4': ExtraTreesClassifier(n_estimators=100, criterion='entropy', max_features='sqrt', max_depth=16, random_state=0),\n",
    "    \n",
    "    'xgb_1': xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               max_depth=3, \n",
    "                               min_child_weight=4, \n",
    "                               gamma=.1,\n",
    "                               n_estimators=500, # slightly more\n",
    "                               learning_rate=0.01,\n",
    "                               reg_alpha=.0, # slight more regularization\n",
    "                               reg_lambda=.1, # slight more regularization\n",
    "                               random_state=0),\n",
    "    'xgb_2': xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               max_depth=3, \n",
    "                               min_child_weight=4, \n",
    "                               gamma=.1,\n",
    "                               n_estimators=500, # slightly more\n",
    "                               learning_rate=0.01,\n",
    "                               reg_alpha=.1, # slight more regularization\n",
    "                               reg_lambda=.2, # slight more regularization\n",
    "                               random_state=0),\n",
    "    'xgb_3': xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               max_depth=3, \n",
    "                               min_child_weight=4, \n",
    "                               gamma=.2,\n",
    "                               n_estimators=500, # slightly more\n",
    "                               learning_rate=0.01,\n",
    "                               reg_alpha=.0, # slight more regularization\n",
    "                               reg_lambda=.1, # slight more regularization\n",
    "                               random_state=0),\n",
    "    'xgb_4': xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               max_depth=3, \n",
    "                               min_child_weight=4, \n",
    "                               gamma=.2,\n",
    "                               n_estimators=500, # slightly more\n",
    "                               learning_rate=0.01,\n",
    "                               reg_alpha=.1, # slight more regularization\n",
    "                               reg_lambda=.2, # slight more regularization\n",
    "                               random_state=0),\n",
    "    'lgb_1': lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 boosting_type='dart',\n",
    "                                 n_estimators=200, # slightly more\n",
    "                                 min_child_samples=5,\n",
    "                                 num_leaves=7, \n",
    "                                 reg_alpha=.4,  # slight more regularization  \n",
    "                                 reg_lambda=.2,  # slight more regularization\n",
    "                                 random_state=0),\n",
    "    'lgb_2': lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 boosting_type='dart',\n",
    "                                 n_estimators=200, # slightly more\n",
    "                                 min_child_samples=5,\n",
    "                                 num_leaves=7, \n",
    "                                 reg_alpha=.45,  # slight more regularization  \n",
    "                                 reg_lambda=.25,  # slight more regularization\n",
    "                                 random_state=0),\n",
    "    'lgb_3': lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 boosting_type='dart',\n",
    "                                 n_estimators=400, # slightly more\n",
    "                                 min_child_samples=5,\n",
    "                                 num_leaves=14, \n",
    "                                 reg_alpha=.4,  # slight more regularization  \n",
    "                                 reg_lambda=.2,  # slight more regularization\n",
    "                                 random_state=0),\n",
    "    'lgb_4': lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 boosting_type='dart',\n",
    "                                 n_estimators=400, # slightly more\n",
    "                                 min_child_samples=5,\n",
    "                                 num_leaves=14, \n",
    "                                 reg_alpha=.45,  # slight more regularization  \n",
    "                                 reg_lambda=.25,  # slight more regularization\n",
    "                                 random_state=0),\n",
    "}\n",
    "\n",
    "semi_sl_base_models_dict = {\n",
    "    #'knn_16': KNeighborsClassifier(n_neighbors=16),\n",
    "    'knn_32': KNeighborsClassifier(n_neighbors=32),\n",
    "    'knn_64': KNeighborsClassifier(n_neighbors=64),\n",
    "    #'knn_128': KNeighborsClassifier(n_neighbors=128)\n",
    "}\n",
    "\n",
    "usl_base_models_dict = {\n",
    "    'kmean_2': KMeans(n_clusters=2),\n",
    "    'kmean_3': KMeans(n_clusters=3),\n",
    "    'kmean_4': KMeans(n_clusters=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sl_base_models_dict = {\n",
    "    'bg_1': BaggingClassifier(n_estimators=200, max_features=.8, max_samples=.8, random_state=0),\n",
    "    'gb_1': GradientBoostingClassifier(n_estimators=200, max_features='sqrt', learning_rate=0.05, \n",
    "                                       max_depth=5, max_leaf_nodes=10, random_state=0),\n",
    "    'xgb_1': xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               max_depth=3, \n",
    "                               min_child_weight=4, \n",
    "                               gamma=.1,\n",
    "                               n_estimators=500, # slightly more\n",
    "                               learning_rate=0.01,\n",
    "                               reg_alpha=.0, # slight more regularization\n",
    "                               reg_lambda=.1, # slight more regularization\n",
    "                               random_state=0),\n",
    "    'xgb_2': xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               max_depth=3, \n",
    "                               min_child_weight=4, \n",
    "                               gamma=.1,\n",
    "                               n_estimators=500, # slightly more\n",
    "                               learning_rate=0.01,\n",
    "                               reg_alpha=.1, # slight more regularization\n",
    "                               reg_lambda=.2, # slight more regularization\n",
    "                               random_state=0),\n",
    "    'lgb_1': lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 boosting_type='dart',\n",
    "                                 n_estimators=200, # slightly more\n",
    "                                 min_child_samples=5,\n",
    "                                 num_leaves=7, \n",
    "                                 reg_alpha=.4,  # slight more regularization  \n",
    "                                 reg_lambda=.2,  # slight more regularization\n",
    "                                 random_state=0),\n",
    "    'lgb_2': lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 boosting_type='dart',\n",
    "                                 n_estimators=200, # slightly more\n",
    "                                 min_child_samples=5,\n",
    "                                 num_leaves=7, \n",
    "                                 reg_alpha=.45,  # slight more regularization  \n",
    "                                 reg_lambda=.25,  # slight more regularization\n",
    "                                 random_state=0),\n",
    "    \n",
    "}\n",
    "\n",
    "semi_sl_base_models_dict = {\n",
    "    #'knn_16': KNeighborsClassifier(n_neighbors=16),\n",
    "    'knn_32': KNeighborsClassifier(n_neighbors=32),\n",
    "    'knn_64': KNeighborsClassifier(n_neighbors=64),\n",
    "    #'knn_128': KNeighborsClassifier(n_neighbors=128)\n",
    "}\n",
    "\n",
    "usl_base_models_dict = {\n",
    "    'kmean_2': KMeans(n_clusters=2),\n",
    "    'kmean_3': KMeans(n_clusters=3),\n",
    "    'kmean_4': KMeans(n_clusters=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sam = StackingAveragedModels(sl_base_models_dict=sl_base_models_dict, \n",
    "                             #semi_sl_base_models_dict=semi_sl_base_models_dict,\n",
    "                             #usl_base_models_dict=usl_base_models_dict,\n",
    "                             meta_model=LogisticRegression(),\n",
    "                             target_col='is_iceberg',\n",
    "                             eval_func=log_loss,\n",
    "                             is_classification=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      " bg_1\n",
      "score= 0.204820228475\n",
      "score= 0.189688933428\n",
      "score= 0.153336684378\n",
      "score= 0.195225919493\n",
      "score= 0.146293384729\n",
      "Avg score =  0.1778730301\n",
      "\n",
      "==================\n",
      " gb_1\n",
      "score= 0.198165104448\n",
      "score= 0.175581511529\n",
      "score= 0.139609412289\n",
      "score= 0.176003706616\n",
      "score= 0.132779120079\n",
      "Avg score =  0.164427770992\n",
      "\n",
      "==================\n",
      " xgb_1\n",
      "score= 0.183222899608\n",
      "score= 0.141988045382\n",
      "score= 0.118549516528\n",
      "score= 0.160629102258\n",
      "score= 0.113158687441\n",
      "Avg score =  0.143509650243\n",
      "\n",
      "==================\n",
      " xgb_2\n",
      "score= 0.183108556501\n",
      "score= 0.142459383075\n",
      "score= 0.119020607417\n",
      "score= 0.160492956197\n",
      "score= 0.113167439307\n",
      "Avg score =  0.143649788499\n",
      "\n",
      "==================\n",
      " lgb_1\n",
      "score= 0.174977791653\n",
      "score= 0.140115903248\n",
      "score= 0.121351527243\n",
      "score= 0.152730429658\n",
      "score= 0.110558125185\n",
      "Avg score =  0.139946755398\n",
      "\n",
      "==================\n",
      " lgb_2\n",
      "score= 0.179091875589\n",
      "score= 0.141311863891\n",
      "score= 0.11528677174\n",
      "score= 0.161439997699\n",
      "score= 0.111992163504\n",
      "Avg score =  0.141824534484\n"
     ]
    }
   ],
   "source": [
    "sam.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = sam.get_meta_train_dataframe(get_dummies=True, pca_enabled=False, pca_variance_th=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 49) (1604,)\n"
     ]
    }
   ],
   "source": [
    "features = new_df.columns.tolist()\n",
    "features.remove('is_iceberg')\n",
    "\n",
    "new_X = new_df[features]\n",
    "new_y = np.array(new_df['is_iceberg']).reshape((new_df.shape[0],))\n",
    "\n",
    "print(new_X.shape, new_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune meta model: lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 1.0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_bin': 255,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 10,\n",
       " 'min_child_weight': 5,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'silent': True,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 50000,\n",
       " 'subsample_freq': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm.LGBMClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'dart', 'n_estimators': 200} -0.138375143321\n"
     ]
    }
   ],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, objective='binary', random_state=0)\n",
    "params = {\n",
    "    'boosting_type': ['gbdt', 'dart'],\n",
    "    'n_estimators': [10, 100, 200, 400, 800, 1200, 1600],\n",
    "    #'learning_rate': [0.1, 0.05, 0.01, 0.005, 0.0001]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_child_samples': 10, 'num_leaves': 31} -0.133331258018\n"
     ]
    }
   ],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='goss',\n",
    "                                 learning_rate=.05,\n",
    "                                 n_estimators=100)\n",
    "params = {\n",
    "    'num_leaves': [2, 3, 7, 15, 31, 63],\n",
    "    'min_child_samples': [1, 2, 3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_split_gain': 0.0} -0.133331258018\n"
     ]
    }
   ],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='goss',\n",
    "                                 learning_rate=.05,\n",
    "                                 n_estimators=100,\n",
    "                                 num_leaves=31,\n",
    "                                 min_child_samples=10)\n",
    "params = {\n",
    "    'min_split_gain': [.0, .1, .2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "b'cannot use bagging in GOSS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-0c860d573bec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlg_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_log_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 638\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    457\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;34m\"\"\"construct booster\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[0;32m   1268\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m                 ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m   1271\u001b[0m             \u001b[1;34m\"\"\"save reference to data\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: b'cannot use bagging in GOSS'"
     ]
    }
   ],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='goss',\n",
    "                                 learning_rate=.05,\n",
    "                                 n_estimators=100,\n",
    "                                 num_leaves=31,\n",
    "                                 min_child_samples=10)\n",
    "\n",
    "params = {\n",
    "    'subsample': [1., .8, .6, .4, .2],\n",
    "    'colsample_bytree': [1., .8, .6, .4, .2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.98, 'subsample': 0.8} -0.136640756114\n"
     ]
    }
   ],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='gbdt',\n",
    "                                 learning_rate=.01,\n",
    "                                 n_estimators=400,\n",
    "                                 num_leaves=3,\n",
    "                                 min_child_samples=1)\n",
    "\n",
    "params = {\n",
    "    'subsample': [.8, .75, .7, .65, .6, .55],\n",
    "    'colsample_bytree': [1., .98, .95, .93, .9]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 0.0, 'reg_lambda': 0.0} -0.136640756114\n"
     ]
    }
   ],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='gbdt',\n",
    "                                 learning_rate=.01,\n",
    "                                 n_estimators=400,\n",
    "                                 num_leaves=3,\n",
    "                                 min_child_samples=1, \n",
    "                                 colsample_bytree=.98,\n",
    "                                 subsample=.8)\n",
    "\n",
    "params = {\n",
    "    'reg_alpha': [1., .8, .6, .4, .2, .1, .0],\n",
    "    'reg_lambda': [1., .8, .6, .4, .2, .1, .0]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 0.025, 'reg_lambda': 0.025} -0.136680693903\n"
     ]
    }
   ],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='gbdt',\n",
    "                                 learning_rate=.01,\n",
    "                                 n_estimators=400,\n",
    "                                 num_leaves=3,\n",
    "                                 min_child_samples=1, \n",
    "                                 colsample_bytree=.98,\n",
    "                                 subsample=.8)\n",
    "\n",
    "params = {\n",
    "    'reg_alpha': [.075, .05, .025],\n",
    "    'reg_lambda': [.075, .05, .025]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_state': 1} 0.956982543641\n"
     ]
    }
   ],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='gbdt',\n",
    "                                 learning_rate=.01,\n",
    "                                 n_estimators=400,\n",
    "                                 num_leaves=3,\n",
    "                                 min_child_samples=1, \n",
    "                                 colsample_bytree=.98,\n",
    "                                 subsample=.8, \n",
    "                                 reg_alpha= 0.025, \n",
    "                                 reg_lambda= 0.025)\n",
    "\n",
    "params = {\n",
    "    'random_state': [0, 1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(lg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 4)\n",
      "Index(['f_0', 'f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_6', 'f_7', 'f_8', 'f_9',\n",
      "       ...\n",
      "       'f_257', 'f_258', 'f_259', 'f_260', 'f_261', 'f_262', 'f_263', 'f_264',\n",
      "       'f_265', 'is_iceberg'],\n",
      "      dtype='object', length=267)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_257</th>\n",
       "      <th>f_258</th>\n",
       "      <th>f_259</th>\n",
       "      <th>f_260</th>\n",
       "      <th>f_261</th>\n",
       "      <th>f_262</th>\n",
       "      <th>f_263</th>\n",
       "      <th>f_264</th>\n",
       "      <th>f_265</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.937001</td>\n",
       "      <td>-0.046966</td>\n",
       "      <td>-0.123941</td>\n",
       "      <td>1.657609</td>\n",
       "      <td>0.387005</td>\n",
       "      <td>-0.935819</td>\n",
       "      <td>0.875177</td>\n",
       "      <td>-1.270511</td>\n",
       "      <td>-0.197012</td>\n",
       "      <td>-0.564091</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.455127</td>\n",
       "      <td>-0.459705</td>\n",
       "      <td>0.570348</td>\n",
       "      <td>0.077780</td>\n",
       "      <td>0.078650</td>\n",
       "      <td>1.267042</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.923009</td>\n",
       "      <td>0.117212</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.118640</td>\n",
       "      <td>-1.240150</td>\n",
       "      <td>0.447019</td>\n",
       "      <td>-1.328023</td>\n",
       "      <td>0.787310</td>\n",
       "      <td>-1.598214</td>\n",
       "      <td>-1.202442</td>\n",
       "      <td>1.852692</td>\n",
       "      <td>-0.389426</td>\n",
       "      <td>-1.097326</td>\n",
       "      <td>...</td>\n",
       "      <td>3.457100</td>\n",
       "      <td>1.954246</td>\n",
       "      <td>-1.251609</td>\n",
       "      <td>-0.835538</td>\n",
       "      <td>0.614450</td>\n",
       "      <td>-0.270183</td>\n",
       "      <td>1.348282</td>\n",
       "      <td>-3.248190</td>\n",
       "      <td>-3.722244</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.764784</td>\n",
       "      <td>3.286722</td>\n",
       "      <td>1.683806</td>\n",
       "      <td>-4.562440</td>\n",
       "      <td>1.846337</td>\n",
       "      <td>4.057018</td>\n",
       "      <td>-1.713150</td>\n",
       "      <td>-2.781449</td>\n",
       "      <td>2.058828</td>\n",
       "      <td>-3.035690</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.944841</td>\n",
       "      <td>-10.129503</td>\n",
       "      <td>2.608267</td>\n",
       "      <td>-12.667193</td>\n",
       "      <td>5.982126</td>\n",
       "      <td>4.465105</td>\n",
       "      <td>-5.248517</td>\n",
       "      <td>20.695408</td>\n",
       "      <td>-0.310907</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.349804</td>\n",
       "      <td>1.111627</td>\n",
       "      <td>-0.250717</td>\n",
       "      <td>0.631179</td>\n",
       "      <td>0.484991</td>\n",
       "      <td>-0.054787</td>\n",
       "      <td>1.577700</td>\n",
       "      <td>-0.875416</td>\n",
       "      <td>0.119568</td>\n",
       "      <td>0.265341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209830</td>\n",
       "      <td>0.366807</td>\n",
       "      <td>1.484570</td>\n",
       "      <td>0.305957</td>\n",
       "      <td>-1.860656</td>\n",
       "      <td>0.660805</td>\n",
       "      <td>-0.159620</td>\n",
       "      <td>0.538565</td>\n",
       "      <td>0.857225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.776327</td>\n",
       "      <td>-0.104925</td>\n",
       "      <td>-1.166807</td>\n",
       "      <td>-0.152499</td>\n",
       "      <td>0.485535</td>\n",
       "      <td>-1.454540</td>\n",
       "      <td>-0.705199</td>\n",
       "      <td>3.206676</td>\n",
       "      <td>-0.383359</td>\n",
       "      <td>-0.126195</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.892168</td>\n",
       "      <td>-2.172888</td>\n",
       "      <td>5.666597</td>\n",
       "      <td>-1.353861</td>\n",
       "      <td>0.659567</td>\n",
       "      <td>0.252173</td>\n",
       "      <td>-4.737852</td>\n",
       "      <td>-0.146233</td>\n",
       "      <td>-1.757885</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 267 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_0       f_1       f_2       f_3       f_4       f_5       f_6  \\\n",
       "0 -0.937001 -0.046966 -0.123941  1.657609  0.387005 -0.935819  0.875177   \n",
       "1  0.118640 -1.240150  0.447019 -1.328023  0.787310 -1.598214 -1.202442   \n",
       "2 -1.764784  3.286722  1.683806 -4.562440  1.846337  4.057018 -1.713150   \n",
       "3  1.349804  1.111627 -0.250717  0.631179  0.484991 -0.054787  1.577700   \n",
       "4 -0.776327 -0.104925 -1.166807 -0.152499  0.485535 -1.454540 -0.705199   \n",
       "\n",
       "        f_7       f_8       f_9     ...         f_257      f_258     f_259  \\\n",
       "0 -1.270511 -0.197012 -0.564091     ...     -1.455127  -0.459705  0.570348   \n",
       "1  1.852692 -0.389426 -1.097326     ...      3.457100   1.954246 -1.251609   \n",
       "2 -2.781449  2.058828 -3.035690     ...     -9.944841 -10.129503  2.608267   \n",
       "3 -0.875416  0.119568  0.265341     ...     -0.209830   0.366807  1.484570   \n",
       "4  3.206676 -0.383359 -0.126195     ...     -2.892168  -2.172888  5.666597   \n",
       "\n",
       "       f_260     f_261     f_262     f_263      f_264     f_265  is_iceberg  \n",
       "0   0.077780  0.078650  1.267042  0.187300   0.923009  0.117212         0.0  \n",
       "1  -0.835538  0.614450 -0.270183  1.348282  -3.248190 -3.722244         0.0  \n",
       "2 -12.667193  5.982126  4.465105 -5.248517  20.695408 -0.310907         0.0  \n",
       "3   0.305957 -1.860656  0.660805 -0.159620   0.538565  0.857225         0.0  \n",
       "4  -1.353861  0.659567  0.252173 -4.737852  -0.146233 -1.757885         0.0  \n",
       "\n",
       "[5 rows x 267 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_json('Data/test.json')\n",
    "print(test_df.shape)\n",
    "test_df.head(5)\n",
    "test_ids = test_df['id']\n",
    "test_df = pd.read_csv('Data/pca_projected_266_from_resnet_test.csv')\n",
    "#test_df.reset_index(drop=True, inplace=True)\n",
    "print(test_df.columns)\n",
    "#test_df.sort_index(inplace=True)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = test_df.columns.tolist()\n",
    "features.remove('is_iceberg')\n",
    "\n",
    "test_X = test_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='gbdt',\n",
    "                                 learning_rate=.01,\n",
    "                                 n_estimators=400,\n",
    "                                 num_leaves=3,\n",
    "                                 min_child_samples=1, \n",
    "                                 colsample_bytree=.98,\n",
    "                                 subsample=.8, \n",
    "                                 reg_alpha= 0.025, \n",
    "                                 reg_lambda= 0.025)\n",
    "sam.reset_meta_model(lg_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta model's training set score=  0.131116933807 \n",
      "\n",
      "[ 0.01722956  0.3750491   0.02713656 ...,  0.01709143  0.98242059\n",
      "  0.02713656]\n"
     ]
    }
   ],
   "source": [
    "predictions = sam.predict_proba(test_X)[:,1]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_ids\n",
    "submission['is_iceberg'] = predictions\n",
    "submission.to_csv('Submissions/submission_sam_lightgbm.csv', float_format=\"%.15f\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> add more regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=3,\n",
    "                                 boosting_type='dart',\n",
    "                                 n_estimators=220, \n",
    "                                 min_child_samples=1,\n",
    "                                 num_leaves=3,\n",
    "                                 min_split_gain=.0, \n",
    "                                 colsample_bytree=.98,\n",
    "                                 subsample=.7, \n",
    "                                 reg_alpha= 0.03, \n",
    "                                 reg_lambda= 0.055)\n",
    "sam.reset_meta_model(lg_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta model's training set score=  0.105920983433 \n",
      "\n",
      "[ 0.00933772  0.26000007  0.03301431 ...,  0.01094177  0.98985594\n",
      "  0.06724654]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Kaggle Competitions\\kaggle-Iceberg-Ship-Classifier-\\stacking_models_api.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  out_of_fold_predictions[self.target_col] = self.out_of_fold_predictions[self.target_col]\n"
     ]
    }
   ],
   "source": [
    "predictions = sam.predict_proba(test_X)[:,1]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_ids\n",
    "submission['is_iceberg'] = predictions\n",
    "submission.to_csv('Submissions/submission_sam_lightgbm_more_regularization.csv', float_format=\"%.15f\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune other meta-model: XGBoost, AdaBoost, Bagging, GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': True,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost.XGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'booster': 'gbtree', 'objective': 'binary:logistic'} -0.138297540138\n"
     ]
    }
   ],
   "source": [
    "xg_clf = xgboost.XGBClassifier(n_jobs=4)\n",
    "params = {\n",
    "    'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "    'objective': ['binary:logistic', 'binary:logitraw']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(xg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 500} -0.135785077947\n"
     ]
    }
   ],
   "source": [
    "xg_clf = xgboost.XGBClassifier(n_jobs=4)\n",
    "params = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [.3, .1, .05, .01, .005, .001]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(xg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.1, 'max_depth': 3, 'min_child_weight': 1} -0.135656785653\n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 0.1, 'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n"
     ]
    }
   ],
   "source": [
    "xg_clf = xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               n_estimators=500,\n",
    "                               learning_rate=0.01)\n",
    "params = {\n",
    "    'max_depth': [0, 3, 5, 7, 9],\n",
    "    'min_child_weight': [.5, 1, 2, 4, 6],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(xg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'subsample': 0.8} -0.13527777042\n"
     ]
    }
   ],
   "source": [
    "xg_clf = xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               n_estimators=500,\n",
    "                               learning_rate=0.01,\n",
    "                               max_depth=3,\n",
    "                               min_child_weight=1,\n",
    "                               gamma=.1)\n",
    "params = {\n",
    "    'colsample_bytree': [1., .8, .6, .4, .2],\n",
    "    'subsample': [1., .8, .6, .4, .2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(xg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'subsample': 0.9} -0.134982915664\n"
     ]
    }
   ],
   "source": [
    "xg_clf = xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               n_estimators=500,\n",
    "                               learning_rate=0.01,\n",
    "                               max_depth=3,\n",
    "                               min_child_weight=1,\n",
    "                               gamma=.1)\n",
    "params = {\n",
    "    'colsample_bytree': [.9, .85, .8, .75, .7],\n",
    "    'subsample': [.9, .85, .8, .75, .7]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(xg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 0.0, 'reg_lambda': 0.05} -0.134864582512\n"
     ]
    }
   ],
   "source": [
    "xg_clf = xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               n_estimators=500,\n",
    "                               learning_rate=0.01,\n",
    "                               max_depth=3,\n",
    "                               min_child_weight=1,\n",
    "                               gamma=.1,\n",
    "                               colsample_bytree=.7,\n",
    "                               subsample=.9)\n",
    "params = {\n",
    "    'reg_alpha': [0., .5, 1.],\n",
    "    'reg_lambda': [1., .5, .1, .05, .01]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(xg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_state': 0} -0.134864582512\n"
     ]
    }
   ],
   "source": [
    "xg_clf = xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               n_estimators=500,\n",
    "                               learning_rate=0.01,\n",
    "                               max_depth=3,\n",
    "                               min_child_weight=1,\n",
    "                               gamma=.1,\n",
    "                               colsample_bytree=.7,\n",
    "                               subsample=.9,\n",
    "                               reg_alpha=.0,\n",
    "                               reg_lambda=.05)\n",
    "params = {\n",
    "    'random_state': [0, 1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(xg_clf, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 400} -0.170063332487\n"
     ]
    }
   ],
   "source": [
    "adb = AdaBoostClassifier(n_estimators=100, learning_rate=.001, random_state=0)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 400, 800]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(adb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001} -0.170063332487\n"
     ]
    }
   ],
   "source": [
    "adb = AdaBoostClassifier(n_estimators=400, learning_rate=.001, random_state=0)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [.001, .005, .0001, .0005]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(adb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap_features': True, 'n_estimators': 400} -0.239761705774\n"
     ]
    }
   ],
   "source": [
    "bg = BaggingClassifier(n_estimators=200, random_state=0, n_jobs=4)\n",
    "\n",
    "params = {\n",
    "    'bootstrap_features': [True, False],\n",
    "    'n_estimators': [100, 200, 300, 400,]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(bg, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1600} -0.220514189297\n"
     ]
    }
   ],
   "source": [
    "bg = BaggingClassifier(n_estimators=200, random_state=0, n_jobs=4, bootstrap_features=True)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [400, 800, 1200, 1600, 2400]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(bg, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 0.6, 'max_samples': 0.6} -0.165277801468\n"
     ]
    }
   ],
   "source": [
    "bg = BaggingClassifier(n_estimators=1600, random_state=0, n_jobs=4, bootstrap_features=True)\n",
    "\n",
    "params = {\n",
    "    'max_samples': [1., .8, .6],\n",
    "    'max_features': [1., .8, .6]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(bg, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 0.7, 'max_samples': 0.5} -0.162197729551\n"
     ]
    }
   ],
   "source": [
    "bg = BaggingClassifier(n_estimators=1600, random_state=0, n_jobs=4, bootstrap_features=True)\n",
    "\n",
    "params = {\n",
    "    'max_samples': [.7, .6, .5],\n",
    "    'max_features': [.7, .6, .5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(bg, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.005, 'n_estimators': 1600} -0.139681349803\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.005, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1600, 'presort': 'auto', 'random_state': 0, 'subsample': 1.0, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=400, max_features='sqrt', learning_rate=0.05, random_state=0)\n",
    "params = {\n",
    "    'n_estimators': [200, 400, 800, 1600, 2400],\n",
    "    'learning_rate': [.1, .05, .01, .005, .001]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'max_leaf_nodes': 5} -0.139233584438\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.005, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': 5, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1600, 'presort': 'auto', 'random_state': 0, 'subsample': 1.0, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=1600, max_features='sqrt', learning_rate=0.005, random_state=0)\n",
    "params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'max_leaf_nodes': [5, 10, 20, None]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_impurity_decrease': 0.1} -0.138504706095\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.005, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': 5, 'min_impurity_decrease': 0.1, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1600, 'presort': 'auto', 'random_state': 0, 'subsample': 1.0, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=1600, max_features='sqrt', \n",
    "                                learning_rate=0.005, random_state=0,\n",
    "                                max_depth=3, max_leaf_nodes=5)\n",
    "params = {\n",
    "    'min_impurity_decrease': [0.0, .1, .2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_weight_fraction_leaf': 0.1} -0.136178455432\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.005, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': 5, 'min_impurity_decrease': 0.1, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.1, 'n_estimators': 1600, 'presort': 'auto', 'random_state': 0, 'subsample': 1.0, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=1600, max_features='sqrt', \n",
    "                                learning_rate=0.005, random_state=0,\n",
    "                                max_depth=3, max_leaf_nodes=5, \n",
    "                                min_impurity_decrease=.1)\n",
    "params = {\n",
    "    'min_weight_fraction_leaf': [.0, .1, .2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.6} -0.134738336622\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.005, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': 5, 'min_impurity_decrease': 0.1, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.1, 'n_estimators': 1600, 'presort': 'auto', 'random_state': 0, 'subsample': 0.6, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=1600, max_features='sqrt', \n",
    "                                learning_rate=0.005, random_state=0,\n",
    "                                max_depth=3, max_leaf_nodes=5, \n",
    "                                min_impurity_decrease=.1, min_weight_fraction_leaf=.1)\n",
    "params = {\n",
    "    'subsample': [1.,.8, .6, .4, .2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.5} -0.134277916917\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.005, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': 5, 'min_impurity_decrease': 0.1, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.1, 'n_estimators': 1600, 'presort': 'auto', 'random_state': 0, 'subsample': 0.5, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=1600, max_features='sqrt', \n",
    "                                learning_rate=0.005, random_state=0,\n",
    "                                max_depth=3, max_leaf_nodes=5, \n",
    "                                min_impurity_decrease=.1, min_weight_fraction_leaf=.1)\n",
    "params = {\n",
    "    'subsample': [.7, .65, .6, .55, .5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.5} -0.134277916917\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.005, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': 5, 'min_impurity_decrease': 0.1, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.1, 'n_estimators': 1600, 'presort': 'auto', 'random_state': 0, 'subsample': 0.5, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=1600, max_features='sqrt', \n",
    "                                learning_rate=0.005, random_state=0,\n",
    "                                max_depth=3, max_leaf_nodes=5, \n",
    "                                min_impurity_decrease=.1, min_weight_fraction_leaf=.1)\n",
    "params = {\n",
    "    'subsample': [.5, .475, .45]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)\n",
    "print(gs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adb = AdaBoostClassifier(n_estimators=400, learning_rate=.001, random_state=0)\n",
    "\n",
    "bg = BaggingClassifier(n_estimators=1600, random_state=0, n_jobs=4, \n",
    "                       bootstrap_features=True, max_features=0.7, max_samples=0.5)\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=1600, max_features='sqrt', \n",
    "                                learning_rate=0.005, random_state=0,\n",
    "                                max_depth=3, max_leaf_nodes=5, \n",
    "                                min_impurity_decrease=.1, min_weight_fraction_leaf=.1,\n",
    "                                subsample=.5)\n",
    "\n",
    "xg_clf = xgboost.XGBClassifier(n_jobs=4, \n",
    "                               booster='gbtree', \n",
    "                               objective='binary:logistic',\n",
    "                               n_estimators=500,\n",
    "                               learning_rate=0.01,\n",
    "                               max_depth=3,\n",
    "                               min_child_weight=1,\n",
    "                               gamma=.1,\n",
    "                               colsample_bytree=.7,\n",
    "                               subsample=.9,\n",
    "                               reg_alpha=.0,\n",
    "                               reg_lambda=.05)\n",
    "\n",
    "lg_clf = lightgbm.LGBMClassifier(n_jobs=4, \n",
    "                                 objective='binary', \n",
    "                                 random_state=0,\n",
    "                                 boosting_type='dart',\n",
    "                                 n_estimators=200, \n",
    "                                 min_child_samples=1,\n",
    "                                 num_leaves=3,\n",
    "                                 min_split_gain=.0, \n",
    "                                 colsample_bytree=.98,\n",
    "                                 subsample=.7, \n",
    "                                 reg_alpha= 0.0, \n",
    "                                 reg_lambda= 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune final blending model: VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': [1, 1, 5, 4, 5]} -0.134376758047\n"
     ]
    }
   ],
   "source": [
    "estimators=[('adb', adb), ('bg',bg), ('gb',gb), ('xg_clf', xg_clf), ('lg_clf', lg_clf)]\n",
    "\n",
    "vc = VotingClassifier(estimators, voting='soft', n_jobs=4)\n",
    "\n",
    "params = {\n",
    "    'weights': [[1,1,3,2,3], [1,1,4,4,4], [1,1,5,4,5]]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(vc, params, scoring='neg_log_loss', cv=5)\n",
    "gs.fit(new_X, new_y)\n",
    "print(gs.best_params_ , gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Kaggle Competitions\\kaggle-Iceberg-Ship-Classifier-\\stacking_models_api.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  out_of_fold_predictions[self.target_col] = self.out_of_fold_predictions[self.target_col]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta model's training set score=  0.0928196276328 \n",
      "\n",
      "[ 0.01155164  0.31794963  0.03674663 ...,  0.0146808   0.98699888\n",
      "  0.06860662]\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators, voting='soft', n_jobs=4, weights=[1,1,5,4,5])\n",
    "sam.reset_meta_model(vc)\n",
    "predictions = sam.predict_proba(test_X)[:,1]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_ids\n",
    "submission['is_iceberg'] = predictions\n",
    "submission.to_csv('Submissions/submission_sam_voting.csv', float_format=\"%.15f\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
