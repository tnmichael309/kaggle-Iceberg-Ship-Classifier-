{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with ResNet in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from resnet import fineTuneResNet50,lessFilterResNet50, resnet50\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 7)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_json('Data/processed_train.json')\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 75, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khyeh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_df.ix[0, 'norm_band_mixed']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model, unused_dict = modResNet18(pretrained=True, skip_headers=['bn2', 'fc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(unused_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "fold_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validate fine-tune @ fc layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==== Pre-trained layers ==== \n",
      "\n",
      "\n",
      "conv1.weight\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.0.conv3.weight\n",
      "layer1.0.bn3.running_mean\n",
      "layer1.0.bn3.running_var\n",
      "layer1.0.bn3.weight\n",
      "layer1.0.bn3.bias\n",
      "layer1.0.downsample.0.weight\n",
      "layer1.0.downsample.1.running_mean\n",
      "layer1.0.downsample.1.running_var\n",
      "layer1.0.downsample.1.weight\n",
      "layer1.0.downsample.1.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.running_mean\n",
      "layer1.1.bn1.running_var\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.running_mean\n",
      "layer1.1.bn2.running_var\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer1.1.conv3.weight\n",
      "layer1.1.bn3.running_mean\n",
      "layer1.1.bn3.running_var\n",
      "layer1.1.bn3.weight\n",
      "layer1.1.bn3.bias\n",
      "layer1.2.conv1.weight\n",
      "layer1.2.bn1.running_mean\n",
      "layer1.2.bn1.running_var\n",
      "layer1.2.bn1.weight\n",
      "layer1.2.bn1.bias\n",
      "layer1.2.conv2.weight\n",
      "layer1.2.bn2.running_mean\n",
      "layer1.2.bn2.running_var\n",
      "layer1.2.bn2.weight\n",
      "layer1.2.bn2.bias\n",
      "layer1.2.conv3.weight\n",
      "layer1.2.bn3.running_mean\n",
      "layer1.2.bn3.running_var\n",
      "layer1.2.bn3.weight\n",
      "layer1.2.bn3.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.conv3.weight\n",
      "layer2.0.bn3.running_mean\n",
      "layer2.0.bn3.running_var\n",
      "layer2.0.bn3.weight\n",
      "layer2.0.bn3.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.running_mean\n",
      "layer2.0.downsample.1.running_var\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.running_mean\n",
      "layer2.1.bn1.running_var\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.running_mean\n",
      "layer2.1.bn2.running_var\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer2.1.conv3.weight\n",
      "layer2.1.bn3.running_mean\n",
      "layer2.1.bn3.running_var\n",
      "layer2.1.bn3.weight\n",
      "layer2.1.bn3.bias\n",
      "layer2.2.conv1.weight\n",
      "layer2.2.bn1.running_mean\n",
      "layer2.2.bn1.running_var\n",
      "layer2.2.bn1.weight\n",
      "layer2.2.bn1.bias\n",
      "layer2.2.conv2.weight\n",
      "layer2.2.bn2.running_mean\n",
      "layer2.2.bn2.running_var\n",
      "layer2.2.bn2.weight\n",
      "layer2.2.bn2.bias\n",
      "layer2.2.conv3.weight\n",
      "layer2.2.bn3.running_mean\n",
      "layer2.2.bn3.running_var\n",
      "layer2.2.bn3.weight\n",
      "layer2.2.bn3.bias\n",
      "layer2.3.conv1.weight\n",
      "layer2.3.bn1.running_mean\n",
      "layer2.3.bn1.running_var\n",
      "layer2.3.bn1.weight\n",
      "layer2.3.bn1.bias\n",
      "layer2.3.conv2.weight\n",
      "layer2.3.bn2.running_mean\n",
      "layer2.3.bn2.running_var\n",
      "layer2.3.bn2.weight\n",
      "layer2.3.bn2.bias\n",
      "layer2.3.conv3.weight\n",
      "layer2.3.bn3.running_mean\n",
      "layer2.3.bn3.running_var\n",
      "layer2.3.bn3.weight\n",
      "layer2.3.bn3.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.conv3.weight\n",
      "layer3.0.bn3.running_mean\n",
      "layer3.0.bn3.running_var\n",
      "layer3.0.bn3.weight\n",
      "layer3.0.bn3.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.running_mean\n",
      "layer3.0.downsample.1.running_var\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.running_mean\n",
      "layer3.1.bn1.running_var\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.running_mean\n",
      "layer3.1.bn2.running_var\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer3.1.conv3.weight\n",
      "layer3.1.bn3.running_mean\n",
      "layer3.1.bn3.running_var\n",
      "layer3.1.bn3.weight\n",
      "layer3.1.bn3.bias\n",
      "layer3.2.conv1.weight\n",
      "layer3.2.bn1.running_mean\n",
      "layer3.2.bn1.running_var\n",
      "layer3.2.bn1.weight\n",
      "layer3.2.bn1.bias\n",
      "layer3.2.conv2.weight\n",
      "layer3.2.bn2.running_mean\n",
      "layer3.2.bn2.running_var\n",
      "layer3.2.bn2.weight\n",
      "layer3.2.bn2.bias\n",
      "layer3.2.conv3.weight\n",
      "layer3.2.bn3.running_mean\n",
      "layer3.2.bn3.running_var\n",
      "layer3.2.bn3.weight\n",
      "layer3.2.bn3.bias\n",
      "layer3.3.conv1.weight\n",
      "layer3.3.bn1.running_mean\n",
      "layer3.3.bn1.running_var\n",
      "layer3.3.bn1.weight\n",
      "layer3.3.bn1.bias\n",
      "layer3.3.conv2.weight\n",
      "layer3.3.bn2.running_mean\n",
      "layer3.3.bn2.running_var\n",
      "layer3.3.bn2.weight\n",
      "layer3.3.bn2.bias\n",
      "layer3.3.conv3.weight\n",
      "layer3.3.bn3.running_mean\n",
      "layer3.3.bn3.running_var\n",
      "layer3.3.bn3.weight\n",
      "layer3.3.bn3.bias\n",
      "layer3.4.conv1.weight\n",
      "layer3.4.bn1.running_mean\n",
      "layer3.4.bn1.running_var\n",
      "layer3.4.bn1.weight\n",
      "layer3.4.bn1.bias\n",
      "layer3.4.conv2.weight\n",
      "layer3.4.bn2.running_mean\n",
      "layer3.4.bn2.running_var\n",
      "layer3.4.bn2.weight\n",
      "layer3.4.bn2.bias\n",
      "layer3.4.conv3.weight\n",
      "layer3.4.bn3.running_mean\n",
      "layer3.4.bn3.running_var\n",
      "layer3.4.bn3.weight\n",
      "layer3.4.bn3.bias\n",
      "layer3.5.conv1.weight\n",
      "layer3.5.bn1.running_mean\n",
      "layer3.5.bn1.running_var\n",
      "layer3.5.bn1.weight\n",
      "layer3.5.bn1.bias\n",
      "layer3.5.conv2.weight\n",
      "layer3.5.bn2.running_mean\n",
      "layer3.5.bn2.running_var\n",
      "layer3.5.bn2.weight\n",
      "layer3.5.bn2.bias\n",
      "layer3.5.conv3.weight\n",
      "layer3.5.bn3.running_mean\n",
      "layer3.5.bn3.running_var\n",
      "layer3.5.bn3.weight\n",
      "layer3.5.bn3.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.conv3.weight\n",
      "layer4.0.bn3.running_mean\n",
      "layer4.0.bn3.running_var\n",
      "layer4.0.bn3.weight\n",
      "layer4.0.bn3.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.running_mean\n",
      "layer4.0.downsample.1.running_var\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.running_mean\n",
      "layer4.1.bn1.running_var\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.running_mean\n",
      "layer4.1.bn2.running_var\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "layer4.1.conv3.weight\n",
      "layer4.1.bn3.running_mean\n",
      "layer4.1.bn3.running_var\n",
      "layer4.1.bn3.weight\n",
      "layer4.1.bn3.bias\n",
      "layer4.2.conv1.weight\n",
      "layer4.2.bn1.running_mean\n",
      "layer4.2.bn1.running_var\n",
      "layer4.2.bn1.weight\n",
      "layer4.2.bn1.bias\n",
      "layer4.2.conv2.weight\n",
      "layer4.2.bn2.running_mean\n",
      "layer4.2.bn2.running_var\n",
      "layer4.2.bn2.weight\n",
      "layer4.2.bn2.bias\n",
      "layer4.2.conv3.weight\n",
      "layer4.2.bn3.running_mean\n",
      "layer4.2.bn3.running_var\n",
      "layer4.2.bn3.weight\n",
      "layer4.2.bn3.bias\n",
      "fc.weight\n",
      "fc.bias\n",
      "\n",
      "\n",
      "====  Loaded layers ==== \n",
      "\n",
      "\n",
      "conv1.weight\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.0.conv3.weight\n",
      "layer1.0.bn3.running_mean\n",
      "layer1.0.bn3.running_var\n",
      "layer1.0.bn3.weight\n",
      "layer1.0.bn3.bias\n",
      "layer1.0.downsample.0.weight\n",
      "layer1.0.downsample.1.running_mean\n",
      "layer1.0.downsample.1.running_var\n",
      "layer1.0.downsample.1.weight\n",
      "layer1.0.downsample.1.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.running_mean\n",
      "layer1.1.bn1.running_var\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.running_mean\n",
      "layer1.1.bn2.running_var\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer1.1.conv3.weight\n",
      "layer1.1.bn3.running_mean\n",
      "layer1.1.bn3.running_var\n",
      "layer1.1.bn3.weight\n",
      "layer1.1.bn3.bias\n",
      "layer1.2.conv1.weight\n",
      "layer1.2.bn1.running_mean\n",
      "layer1.2.bn1.running_var\n",
      "layer1.2.bn1.weight\n",
      "layer1.2.bn1.bias\n",
      "layer1.2.conv2.weight\n",
      "layer1.2.bn2.running_mean\n",
      "layer1.2.bn2.running_var\n",
      "layer1.2.bn2.weight\n",
      "layer1.2.bn2.bias\n",
      "layer1.2.conv3.weight\n",
      "layer1.2.bn3.running_mean\n",
      "layer1.2.bn3.running_var\n",
      "layer1.2.bn3.weight\n",
      "layer1.2.bn3.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.conv3.weight\n",
      "layer2.0.bn3.running_mean\n",
      "layer2.0.bn3.running_var\n",
      "layer2.0.bn3.weight\n",
      "layer2.0.bn3.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.running_mean\n",
      "layer2.0.downsample.1.running_var\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.running_mean\n",
      "layer2.1.bn1.running_var\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.running_mean\n",
      "layer2.1.bn2.running_var\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer2.1.conv3.weight\n",
      "layer2.1.bn3.running_mean\n",
      "layer2.1.bn3.running_var\n",
      "layer2.1.bn3.weight\n",
      "layer2.1.bn3.bias\n",
      "layer2.2.conv1.weight\n",
      "layer2.2.bn1.running_mean\n",
      "layer2.2.bn1.running_var\n",
      "layer2.2.bn1.weight\n",
      "layer2.2.bn1.bias\n",
      "layer2.2.conv2.weight\n",
      "layer2.2.bn2.running_mean\n",
      "layer2.2.bn2.running_var\n",
      "layer2.2.bn2.weight\n",
      "layer2.2.bn2.bias\n",
      "layer2.2.conv3.weight\n",
      "layer2.2.bn3.running_mean\n",
      "layer2.2.bn3.running_var\n",
      "layer2.2.bn3.weight\n",
      "layer2.2.bn3.bias\n",
      "layer2.3.conv1.weight\n",
      "layer2.3.bn1.running_mean\n",
      "layer2.3.bn1.running_var\n",
      "layer2.3.bn1.weight\n",
      "layer2.3.bn1.bias\n",
      "layer2.3.conv2.weight\n",
      "layer2.3.bn2.running_mean\n",
      "layer2.3.bn2.running_var\n",
      "layer2.3.bn2.weight\n",
      "layer2.3.bn2.bias\n",
      "layer2.3.conv3.weight\n",
      "layer2.3.bn3.running_mean\n",
      "layer2.3.bn3.running_var\n",
      "layer2.3.bn3.weight\n",
      "layer2.3.bn3.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.conv3.weight\n",
      "layer3.0.bn3.running_mean\n",
      "layer3.0.bn3.running_var\n",
      "layer3.0.bn3.weight\n",
      "layer3.0.bn3.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.running_mean\n",
      "layer3.0.downsample.1.running_var\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.running_mean\n",
      "layer3.1.bn1.running_var\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.running_mean\n",
      "layer3.1.bn2.running_var\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer3.1.conv3.weight\n",
      "layer3.1.bn3.running_mean\n",
      "layer3.1.bn3.running_var\n",
      "layer3.1.bn3.weight\n",
      "layer3.1.bn3.bias\n",
      "layer3.2.conv1.weight\n",
      "layer3.2.bn1.running_mean\n",
      "layer3.2.bn1.running_var\n",
      "layer3.2.bn1.weight\n",
      "layer3.2.bn1.bias\n",
      "layer3.2.conv2.weight\n",
      "layer3.2.bn2.running_mean\n",
      "layer3.2.bn2.running_var\n",
      "layer3.2.bn2.weight\n",
      "layer3.2.bn2.bias\n",
      "layer3.2.conv3.weight\n",
      "layer3.2.bn3.running_mean\n",
      "layer3.2.bn3.running_var\n",
      "layer3.2.bn3.weight\n",
      "layer3.2.bn3.bias\n",
      "layer3.3.conv1.weight\n",
      "layer3.3.bn1.running_mean\n",
      "layer3.3.bn1.running_var\n",
      "layer3.3.bn1.weight\n",
      "layer3.3.bn1.bias\n",
      "layer3.3.conv2.weight\n",
      "layer3.3.bn2.running_mean\n",
      "layer3.3.bn2.running_var\n",
      "layer3.3.bn2.weight\n",
      "layer3.3.bn2.bias\n",
      "layer3.3.conv3.weight\n",
      "layer3.3.bn3.running_mean\n",
      "layer3.3.bn3.running_var\n",
      "layer3.3.bn3.weight\n",
      "layer3.3.bn3.bias\n",
      "layer3.4.conv1.weight\n",
      "layer3.4.bn1.running_mean\n",
      "layer3.4.bn1.running_var\n",
      "layer3.4.bn1.weight\n",
      "layer3.4.bn1.bias\n",
      "layer3.4.conv2.weight\n",
      "layer3.4.bn2.running_mean\n",
      "layer3.4.bn2.running_var\n",
      "layer3.4.bn2.weight\n",
      "layer3.4.bn2.bias\n",
      "layer3.4.conv3.weight\n",
      "layer3.4.bn3.running_mean\n",
      "layer3.4.bn3.running_var\n",
      "layer3.4.bn3.weight\n",
      "layer3.4.bn3.bias\n",
      "layer3.5.conv1.weight\n",
      "layer3.5.bn1.running_mean\n",
      "layer3.5.bn1.running_var\n",
      "layer3.5.bn1.weight\n",
      "layer3.5.bn1.bias\n",
      "layer3.5.conv2.weight\n",
      "layer3.5.bn2.running_mean\n",
      "layer3.5.bn2.running_var\n",
      "layer3.5.bn2.weight\n",
      "layer3.5.bn2.bias\n",
      "layer3.5.conv3.weight\n",
      "layer3.5.bn3.running_mean\n",
      "layer3.5.bn3.running_var\n",
      "layer3.5.bn3.weight\n",
      "layer3.5.bn3.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.conv3.weight\n",
      "layer4.0.bn3.running_mean\n",
      "layer4.0.bn3.running_var\n",
      "layer4.0.bn3.weight\n",
      "layer4.0.bn3.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.running_mean\n",
      "layer4.0.downsample.1.running_var\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.running_mean\n",
      "layer4.1.bn1.running_var\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.running_mean\n",
      "layer4.1.bn2.running_var\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "layer4.1.conv3.weight\n",
      "layer4.1.bn3.running_mean\n",
      "layer4.1.bn3.running_var\n",
      "layer4.1.bn3.weight\n",
      "layer4.1.bn3.bias\n",
      "layer4.2.conv1.weight\n",
      "layer4.2.bn1.running_mean\n",
      "layer4.2.bn1.running_var\n",
      "layer4.2.bn1.weight\n",
      "layer4.2.bn1.bias\n",
      "layer4.2.conv2.weight\n",
      "layer4.2.bn2.running_mean\n",
      "layer4.2.bn2.running_var\n",
      "layer4.2.bn2.weight\n",
      "layer4.2.bn2.bias\n",
      "layer4.2.conv3.weight\n",
      "layer4.2.bn3.running_mean\n",
      "layer4.2.bn3.running_var\n",
      "layer4.2.bn3.weight\n",
      "layer4.2.bn3.bias\n",
      "fc.weight\n",
      "fc.bias\n",
      "Dict. to train:\n",
      "dict_keys(['bn2.weight', 'bn2.bias', 'bn2.running_mean', 'bn2.running_var', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "conv1.weight bn1.weight bn1.bias layer1.0.conv1.weight layer1.0.bn1.weight layer1.0.bn1.bias layer1.0.conv2.weight layer1.0.bn2.weight layer1.0.bn2.bias layer1.0.conv3.weight layer1.0.bn3.weight layer1.0.bn3.bias layer1.0.downsample.0.weight layer1.0.downsample.1.weight layer1.0.downsample.1.bias layer1.1.conv1.weight layer1.1.bn1.weight layer1.1.bn1.bias layer1.1.conv2.weight layer1.1.bn2.weight layer1.1.bn2.bias layer1.1.conv3.weight layer1.1.bn3.weight layer1.1.bn3.bias layer1.2.conv1.weight layer1.2.bn1.weight layer1.2.bn1.bias layer1.2.conv2.weight layer1.2.bn2.weight layer1.2.bn2.bias layer1.2.conv3.weight layer1.2.bn3.weight layer1.2.bn3.bias layer2.0.conv1.weight layer2.0.bn1.weight layer2.0.bn1.bias layer2.0.conv2.weight layer2.0.bn2.weight layer2.0.bn2.bias layer2.0.conv3.weight layer2.0.bn3.weight layer2.0.bn3.bias layer2.0.downsample.0.weight layer2.0.downsample.1.weight layer2.0.downsample.1.bias layer2.1.conv1.weight layer2.1.bn1.weight layer2.1.bn1.bias layer2.1.conv2.weight layer2.1.bn2.weight layer2.1.bn2.bias layer2.1.conv3.weight layer2.1.bn3.weight layer2.1.bn3.bias layer2.2.conv1.weight layer2.2.bn1.weight layer2.2.bn1.bias layer2.2.conv2.weight layer2.2.bn2.weight layer2.2.bn2.bias layer2.2.conv3.weight layer2.2.bn3.weight layer2.2.bn3.bias layer2.3.conv1.weight layer2.3.bn1.weight layer2.3.bn1.bias layer2.3.conv2.weight layer2.3.bn2.weight layer2.3.bn2.bias layer2.3.conv3.weight layer2.3.bn3.weight layer2.3.bn3.bias layer3.0.conv1.weight layer3.0.bn1.weight layer3.0.bn1.bias layer3.0.conv2.weight layer3.0.bn2.weight layer3.0.bn2.bias layer3.0.conv3.weight layer3.0.bn3.weight layer3.0.bn3.bias layer3.0.downsample.0.weight layer3.0.downsample.1.weight layer3.0.downsample.1.bias layer3.1.conv1.weight layer3.1.bn1.weight layer3.1.bn1.bias layer3.1.conv2.weight layer3.1.bn2.weight layer3.1.bn2.bias layer3.1.conv3.weight layer3.1.bn3.weight layer3.1.bn3.bias layer3.2.conv1.weight layer3.2.bn1.weight layer3.2.bn1.bias layer3.2.conv2.weight layer3.2.bn2.weight layer3.2.bn2.bias layer3.2.conv3.weight layer3.2.bn3.weight layer3.2.bn3.bias layer3.3.conv1.weight layer3.3.bn1.weight layer3.3.bn1.bias layer3.3.conv2.weight layer3.3.bn2.weight layer3.3.bn2.bias layer3.3.conv3.weight layer3.3.bn3.weight layer3.3.bn3.bias layer3.4.conv1.weight layer3.4.bn1.weight layer3.4.bn1.bias layer3.4.conv2.weight layer3.4.bn2.weight layer3.4.bn2.bias layer3.4.conv3.weight layer3.4.bn3.weight layer3.4.bn3.bias layer3.5.conv1.weight layer3.5.bn1.weight layer3.5.bn1.bias layer3.5.conv2.weight layer3.5.bn2.weight layer3.5.bn2.bias layer3.5.conv3.weight layer3.5.bn3.weight layer3.5.bn3.bias layer4.0.conv1.weight layer4.0.bn1.weight layer4.0.bn1.bias layer4.0.conv2.weight layer4.0.bn2.weight layer4.0.bn2.bias layer4.0.conv3.weight layer4.0.bn3.weight layer4.0.bn3.bias layer4.0.downsample.0.weight layer4.0.downsample.1.weight layer4.0.downsample.1.bias layer4.1.conv1.weight layer4.1.bn1.weight layer4.1.bn1.bias layer4.1.conv2.weight layer4.1.bn2.weight layer4.1.bn2.bias layer4.1.conv3.weight layer4.1.bn3.weight layer4.1.bn3.bias layer4.2.conv1.weight layer4.2.bn1.weight layer4.2.bn1.bias layer4.2.conv2.weight layer4.2.bn2.weight layer4.2.bn2.bias layer4.2.conv3.weight layer4.2.bn3.weight layer4.2.bn3.bias fc.weight fc.bias \n",
      "Train params: bn2.weight\n",
      "\n",
      "Train params: bn2.bias\n",
      "\n",
      "Train params: fc2.weight\n",
      "\n",
      "Train params: fc2.bias\n",
      "\n",
      "Train params: fc3.weight\n",
      "\n",
      "Train params: fc3.bias\n",
      "gpu: 0  available: True\n",
      "Model in use:\n",
      "FineTuneResNet (\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu): ReLU (inplace)\n",
      "  (maxpool): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  (layer1): Sequential (\n",
      "    (0): Bottleneck (\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck (\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "    (2): Bottleneck (\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential (\n",
      "    (0): Bottleneck (\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck (\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "    (2): Bottleneck (\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "    (3): Bottleneck (\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential (\n",
      "    (0): Bottleneck (\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck (\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "    (2): Bottleneck (\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "    (3): Bottleneck (\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "    (4): Bottleneck (\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "    (5): Bottleneck (\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential (\n",
      "    (0): Bottleneck (\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck (\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "    (2): Bottleneck (\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d (size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  (fc): Linear (2048 -> 1000)\n",
      "  (bn2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc2): Linear (1001 -> 30)\n",
      "  (fc3): Linear (30 -> 1)\n",
      "  (prob): Sigmoid ()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10: Train set: Average loss: 0.5923, Accuracy: 924/1283 (72.02%)\n",
      "Valid set: Average loss: 0.5505, Accuracy: 229/321 (71.34%)\n",
      "epoch= 20: Train set: Average loss: 0.5006, Accuracy: 955/1283 (74.43%)\n",
      "Valid set: Average loss: 0.5067, Accuracy: 234/321 (72.90%)\n",
      "epoch= 30: Train set: Average loss: 0.5622, Accuracy: 933/1283 (72.72%)\n",
      "Valid set: Average loss: 0.5028, Accuracy: 230/321 (71.65%)\n",
      "epoch= 40: Train set: Average loss: 0.4924, Accuracy: 957/1283 (74.59%)\n",
      "Valid set: Average loss: 0.4937, Accuracy: 230/321 (71.65%)\n",
      "epoch= 50: Train set: Average loss: 0.5002, Accuracy: 933/1283 (72.72%)\n",
      "Valid set: Average loss: 0.5134, Accuracy: 235/321 (73.21%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1dec128bf3aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_transfer_learning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_transfer_learning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Kaggle Competitions\\kaggle-Iceberg-Ship-Classifier-\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_df, valid_df, is_transfer_learning)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_show\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_show\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Kaggle Competitions\\kaggle-Iceberg-Ship-Classifier-\\trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, data_loader, is_show)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=fold_num, shuffle=True, random_state=519)\n",
    "counter  = 0\n",
    "mean_err = 0\n",
    "\n",
    "for train_index, valid_index in kf.split(train_df):\n",
    "     \n",
    "    is_transfer_learning = True\n",
    "    model, to_train_dict = fineTuneResNet50(pretrained=True)\n",
    "\n",
    "    print(\"Dict. to train:\")\n",
    "    print(to_train_dict.keys())\n",
    "    \n",
    "    # set only the parameters in to_train_dict to train\n",
    "    # others: requires_grad = False\n",
    "    to_train_parmas = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in to_train_dict.keys():\n",
    "            print('\\nTrain params:', name)\n",
    "            to_train_parmas.append(param)\n",
    "        else:\n",
    "            print(name, end=\" \")\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    optimizer = torch.optim.Adam(to_train_parmas, lr=0.001, weight_decay=0.001)\n",
    "    \n",
    "    '''\n",
    "    is_transfer_learning = False\n",
    "    model = lessFilterResNet50()\n",
    "    # testing snap shot for essemble\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    '''\n",
    "    tr = Trainer(\n",
    "            model,\n",
    "            optimizer,\n",
    "            epochs=1000,\n",
    "            milestones=[50, 600],\n",
    "            gamma=0.1,\n",
    "            batch_size=128, \n",
    "            use_cuda=True, \n",
    "            gpu_idx=0)\n",
    "    train = train_df.loc[train_index].reset_index(drop=True)\n",
    "    valid = train_df.loc[valid_index].reset_index(drop=True)\n",
    "\n",
    "    tr.train(train, valid_df=valid, is_transfer_learning=is_transfer_learning)\n",
    "    \n",
    "    counter += 1\n",
    "    tr.save('Trained_model/fine_tune_resent50_' + str(counter) + '.db')\n",
    "    del model, tr, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validate less filter ResNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=fold_num, shuffle=True, random_state=519)\n",
    "counter  = 0\n",
    "mean_err = 0\n",
    "\n",
    "for train_index, valid_index in kf.split(train_df):\n",
    "     \n",
    "    \n",
    "    is_transfer_learning = False\n",
    "    model = lessFilterResNet50()\n",
    "    # testing snap shot for essemble\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    \n",
    "    tr = Trainer(\n",
    "            model,\n",
    "            optimizer,\n",
    "            epochs=1000,\n",
    "            milestones=[50, 600],\n",
    "            gamma=0.1,\n",
    "            batch_size=128, \n",
    "            use_cuda=True, \n",
    "            gpu_idx=0)\n",
    "    train = train_df.loc[train_index].reset_index(drop=True)\n",
    "    valid = train_df.loc[valid_index].reset_index(drop=True)\n",
    "\n",
    "    tr.train(train, valid_df=valid, is_transfer_learning=is_transfer_learning)\n",
    "    \n",
    "    counter += 1\n",
    "    tr.save('Trained_model/fine_tune_resent50_' + str(counter) + '.db')\n",
    "    del model, tr, optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
