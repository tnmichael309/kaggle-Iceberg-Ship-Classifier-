{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, SparsePCA, MiniBatchSparsePCA, KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "kfold = KFold(5, shuffle=True, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_df():\n",
    "    train_X = pd.read_json('Data/sswae_gen_train.json')\n",
    "    train_X.sort_index(inplace=True)\n",
    "    features = train_X.columns.tolist()\n",
    "    features.remove('is_iceberg')\n",
    "    train_y = np.array(train_X['is_iceberg']).reshape((train_X.shape[0],))\n",
    "    \n",
    "    train_X = train_X[features]\n",
    "    \n",
    "    print(\"Train data X: {}, y: {}\".format(train_X.shape, train_y.shape))\n",
    "    return train_X, train_y, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_df(cols=None):\n",
    "    test_df = pd.read_json('Data/sswae_gen_test.json')\n",
    "    test_df.sort_index(inplace=True)\n",
    "        \n",
    "    print(\"Test data X: {}\".format(test_df.shape))\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_num(pca, th):\n",
    "    counts = pd.Series(pca.explained_variance_ >= th).value_counts(sort=False)\n",
    "    return counts.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transformed_df(pca, df):\n",
    "    new_data = np.array(pca.transform(df))\n",
    "    columns = ['f_{}'.format(i) for i in range(new_data.shape[1])]\n",
    "    df = pd.DataFrame(data=new_data,    # values\n",
    "                  index=df.index,\n",
    "                  columns=columns)  # 1st row as the column names\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_log_loss(fn, df, y):\n",
    "    pca = PCA(whiten=False, svd_solver='randomized', random_state=0, n_components=fn)\n",
    "    pca.fit(df)\n",
    "    \n",
    "    X = get_transformed_df(pca, df)\n",
    "    \n",
    "    clf = lightgbm.LGBMClassifier(n_jobs=4, objective='binary', random_state=0, n_estimators=100, boosting_type='dart')\n",
    "    log_loss = -1*cross_val_score(clf, X=X, y=y, scoring='neg_log_loss', cv=kfold)\n",
    "    avg_log_loss = sum(log_loss) / float(len(log_loss))\n",
    "    \n",
    "    return avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_selection(train_X, train_y):\n",
    "    pca = PCA(whiten=False, svd_solver='randomized', random_state=0)\n",
    "    pca.fit(train_X)\n",
    "    \n",
    "    th_list = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
    "    f_num = [get_feature_num(pca, th) for th in th_list]\n",
    "    \n",
    "    print(\"# of features to test: \",f_num)\n",
    "    cv_scores = [get_log_loss(fn, train_X, train_y) for fn in f_num]\n",
    "    \n",
    "    index_min = np.argmin(cv_scores)\n",
    "    print(\"# of features by light gbm: {}, cv score: {:.3f}\".format(f_num[index_min], cv_scores[index_min]))\n",
    "    \n",
    "    pca = PCA(whiten=False, svd_solver='randomized', random_state=0, n_components=f_num[index_min])\n",
    "    pca.fit(train_X)\n",
    "    train_X = get_transformed_df(pca, train_X)\n",
    "    \n",
    "    print(\"New train data X: {}, y: {}\".format(train_X.shape, train_y.shape))\n",
    "    return train_X, train_y, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class base_tuner():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.default_params = {}\n",
    "        \n",
    "    def fit_and_update_params(self, params, update=True):\n",
    "        clf = self.get_clf()\n",
    "\n",
    "        gs = GridSearchCV(clf, params, scoring='neg_log_loss', cv=kfold, return_train_score=False)\n",
    "        gs.fit(self.X, self.y)\n",
    "        \n",
    "        cv_df = pd.DataFrame().from_dict(gs.cv_results_)\n",
    "        cv_df = cv_df[['mean_test_score', 'std_test_score', 'params', 'rank_test_score']]\n",
    "        cv_df = cv_df.sort_values(by=['rank_test_score', 'std_test_score']).reset_index(drop=True)\n",
    "        best_params = cv_df.loc[0, 'params']\n",
    "        \n",
    "        if update is True:\n",
    "            self.default_params.update(best_params)\n",
    "        \n",
    "        print('Selected hyper-params:', best_params)\n",
    "        print('==============================> cv score: {:.4f}'.format(cv_df.loc[0, 'mean_test_score']))\n",
    "        return best_params\n",
    "    \n",
    "    def tune(self):\n",
    "        pass\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lgbm_tuner(base_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(lgbm_tuner, self).__init__(X, y)\n",
    "        self.default_params = {\n",
    "            'n_jobs': 4,\n",
    "            'objective': 'binary',\n",
    "            'random_state': 0,\n",
    "            'boosting_type': 'dart'\n",
    "        }\n",
    "    \n",
    "    def tune_est_num_and_lr(self):\n",
    "        params = {\n",
    "            'n_estimators': [100, 200, 400, 800],\n",
    "            'learning_rate': [0.1, 0.05, 0.01, 0.005]\n",
    "        }\n",
    "\n",
    "        self.fit_and_update_params(params)\n",
    "   \n",
    "    def tune_leaves_num_and_gamma(self):\n",
    "        params = {\n",
    "            'num_leaves': [2, 3, 7, 15, 31, 63],\n",
    "            'min_split_gain': [.0, .1, .2]\n",
    "        }\n",
    "        self.fit_and_update_params(params)\n",
    "     \n",
    "    def tune_sampling(self):\n",
    "        params = {\n",
    "            'subsample': [1., .8, .6, .4, .2],\n",
    "            'colsample_bytree': [1., .8, .6, .4, .2]\n",
    "        }\n",
    "        best_parmas = self.fit_and_update_params(params, update=False)\n",
    "        \n",
    "        next_params = {}\n",
    "        for k,v in best_parmas.items():\n",
    "            if v == 1.:\n",
    "                next_params[k] = [1., .95, .9, .85]\n",
    "            else:\n",
    "                next_params[k] = [v+.15, v+.1, v+.05, v, v-.05, v-.1, v-.15]\n",
    " \n",
    "        self.fit_and_update_params(next_params)\n",
    "        \n",
    "    def tune_regularization(self):\n",
    "        params = {\n",
    "            'reg_alpha': [1., .8, .6, .4, .2, .0],\n",
    "            'reg_lambda': [1., .8, .6, .4, .2, .0]\n",
    "        }\n",
    "        \n",
    "        best_parmas = self.fit_and_update_params(params, update=False)\n",
    "        \n",
    "        next_params = {}\n",
    "        for k,v in best_parmas.items():\n",
    "            if v == 1.:\n",
    "                next_params[k] = [1., .95, .9, .85]\n",
    "            elif v == 0.:\n",
    "                next_params[k] = [.0, .05, .1, .15]\n",
    "            else:\n",
    "                next_params[k] = [v+.15, v+.1, v+.05, v, v-.05, v-.1, v-.15]\n",
    " \n",
    "        self.fit_and_update_params(next_params)\n",
    "    \n",
    "    def tune(self):\n",
    "        print('lgb tuner start tuning')\n",
    "        self.tune_est_num_and_lr()\n",
    "        self.tune_leaves_num_and_gamma()\n",
    "        self.tune_sampling()\n",
    "        self.tune_regularization()\n",
    "        \n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return lightgbm.LGBMClassifier(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class xgb_tuner(base_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(xgb_tuner, self).__init__(X, y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.default_params = {\n",
    "            'n_jobs': 4,\n",
    "            'objective': 'binary:logistic',\n",
    "            'seed': 0,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "    \n",
    "    def tune_booster(self):\n",
    "        params = {\n",
    "            'booster': ['dart', 'gbtree']\n",
    "        }\n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "    def tune_est_num_and_lr(self):\n",
    "        params = {\n",
    "            'n_estimators': [100, 200, 400, 800],\n",
    "            'learning_rate': [0.1, 0.05, 0.01, 0.005]\n",
    "        }\n",
    "\n",
    "        self.fit_and_update_params(params)\n",
    "   \n",
    "    def tune_max_depth(self):\n",
    "        params = {\n",
    "            'max_depth': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "    def tune_child_w_and_gamma(self):\n",
    "        params = {\n",
    "            'min_child_weight': [1, 2, 4, 6, 8, 10],\n",
    "            'gamma': [0, 0.1, 0.2]\n",
    "        }\n",
    "        self.fit_and_update_params(params)\n",
    "     \n",
    "    def tune_sampling(self):\n",
    "        params = {\n",
    "            'subsample': [1., .8, .6, .4, .2],\n",
    "            'colsample_bytree': [1., .8, .6, .4, .2]\n",
    "        }\n",
    "        best_parmas = self.fit_and_update_params(params, update=False)\n",
    "        \n",
    "        next_params = {}\n",
    "        for k,v in best_parmas.items():\n",
    "            if v == 1.:\n",
    "                next_params[k] = [1., .95, .9, .85]\n",
    "            else:\n",
    "                next_params[k] = [v+.15, v+.1, v+.05, v, v-.05, v-.1, v-.15]\n",
    " \n",
    "        self.fit_and_update_params(next_params)\n",
    "        \n",
    "    def tune_regularization(self):\n",
    "        params = {\n",
    "            'reg_alpha': [1., .8, .6, .4, .2, .0],\n",
    "            'reg_lambda': [1., .8, .6, .4, .2, .0]\n",
    "        }\n",
    "        \n",
    "        best_parmas = self.fit_and_update_params(params, update=False)\n",
    "        \n",
    "        next_params = {}\n",
    "        for k,v in best_parmas.items():\n",
    "            if v == 1.:\n",
    "                next_params[k] = [1., .95, .9, .85]\n",
    "            elif v == 0.:\n",
    "                next_params[k] = [.0, .05, .1, .15]\n",
    "            else:\n",
    "                next_params[k] = [v+.15, v+.1, v+.05, v, v-.05, v-.1, v-.15]\n",
    " \n",
    "        self.fit_and_update_params(next_params)\n",
    "    \n",
    "    def tune(self):\n",
    "        print('xgb tuner start tuning')\n",
    "        self.tune_booster()\n",
    "        self.tune_est_num_and_lr()\n",
    "        self.tune_max_depth()\n",
    "        self.tune_child_w_and_gamma()\n",
    "        self.tune_sampling()\n",
    "        self.tune_regularization()\n",
    "        \n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return xgboost.XGBClassifier(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lr_tuner(base_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(lr_tuner, self).__init__(X, y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.default_params = {\n",
    "            'penalty': 'l1',\n",
    "            'max_iter': 10000\n",
    "        }\n",
    "\n",
    "    def tune(self):\n",
    "        print('logistic regression tuner start tuning')\n",
    "        params = {\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "        \n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return LogisticRegression(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mlp_tuner(base_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(mlp_tuner, self).__init__(X, y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.default_params = {\n",
    "            'learning_rate': 'adaptive',\n",
    "            'learning_rate_init': 0.005,\n",
    "            'max_iter': 2000,\n",
    "            'random_state':0\n",
    "        }\n",
    "\n",
    "    def tune(self):\n",
    "        print('mlp tuner start tuning')\n",
    "        params = {\n",
    "            'solver':['lbfgs', 'sgd', 'adam'],\n",
    "            'hidden_layer_sizes': [(100,), (150,), (100, 100,)],\n",
    "        }\n",
    "        \n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "        params = {\n",
    "            'alpha': [10., 5., 2., 1., .8, .5, .2, .1],\n",
    "        }\n",
    "        \n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return MLPClassifier(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class adb_tuner(base_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(adb_tuner, self).__init__(X, y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.default_params = {\n",
    "            'algorithm': 'SAMME.R',\n",
    "            'random_state':0\n",
    "        }\n",
    "\n",
    "    def tune(self):\n",
    "        print('adaboost tuner start tuning')\n",
    "        params = {\n",
    "            'n_estimators': [100, 200, 400, 800],\n",
    "            'learning_rate': [0.1, 0.05, 0.01, 0.005]\n",
    "        }\n",
    "        \n",
    "        self.fit_and_update_params(params)\n",
    "\n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return AdaBoostClassifier(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bg_tuner(base_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(bg_tuner, self).__init__(X, y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.default_params = {\n",
    "            'random_state':0,\n",
    "            'bootstrap_features':True\n",
    "        }\n",
    "        \n",
    "    def tune_est_num(self):\n",
    "        params = {\n",
    "            'n_estimators': [100, 200, 400, 800]\n",
    "        }\n",
    "\n",
    "        self.fit_and_update_params(params)\n",
    "     \n",
    "    def tune_sampling(self):\n",
    "        params = {\n",
    "            'max_samples': [1., .8, .6, .4, .2],\n",
    "            'max_features': [1., .8, .6, .4, .2]\n",
    "        }\n",
    "        best_parmas = self.fit_and_update_params(params, update=False)\n",
    "        \n",
    "        next_params = {}\n",
    "        for k,v in best_parmas.items():\n",
    "            if v == 1.:\n",
    "                next_params[k] = [1., .95, .9, .85]\n",
    "            else:\n",
    "                next_params[k] = [v+.15, v+.1, v+.05, v, v-.05, v-.1, v-.15]\n",
    " \n",
    "        self.fit_and_update_params(next_params)\n",
    "    \n",
    "    def tune(self):\n",
    "        print('bagging tuner start tuning')\n",
    "        self.tune_est_num()\n",
    "        self.tune_sampling()\n",
    "\n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return BaggingClassifier(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gb_tuner(base_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(gb_tuner, self).__init__(X, y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.default_params = {\n",
    "            'random_state':0\n",
    "        }\n",
    "        \n",
    "    def tune_loss_criterion(self):\n",
    "        params = {\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'criterion': ['friedman_mse', 'mse']\n",
    "        }\n",
    "\n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "    def tune_est_num_and_lr(self):\n",
    "        params = {\n",
    "            'n_estimators': [100, 200, 400, 800],\n",
    "            'learning_rate': [0.1, 0.05, 0.01, 0.005]\n",
    "        }\n",
    "\n",
    "        self.fit_and_update_params(params)\n",
    "   \n",
    "    def tune_max_depth(self):\n",
    "        params = {\n",
    "            'max_depth': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "    def tune_child(self):\n",
    "        params = {\n",
    "            'min_samples_split': [2, 3, 7, 15,31],\n",
    "            'min_impurity_decrease': [0, 0.1, 0.2]\n",
    "        }\n",
    "        self.fit_and_update_params(params)\n",
    "     \n",
    "    def tune_sampling(self):\n",
    "        params = {\n",
    "            'subsample': [1., .8, .6, .4, .2],\n",
    "            'max_features': [1., .8, .6, .4, .2]\n",
    "        }\n",
    "        best_parmas = self.fit_and_update_params(params, update=False)\n",
    "        \n",
    "        next_params = {}\n",
    "        for k,v in best_parmas.items():\n",
    "            if v == 1.:\n",
    "                next_params[k] = [1., .95, .9, .85]\n",
    "            else:\n",
    "                next_params[k] = [v+.15, v+.1, v+.05, v, v-.05, v-.1, v-.15]\n",
    " \n",
    "        self.fit_and_update_params(next_params)\n",
    "      \n",
    "    def tune(self):\n",
    "        print('gradient boosting tuner start tuning')\n",
    "        self.tune_loss_criterion()\n",
    "        self.tune_est_num_and_lr()\n",
    "        self.tune_max_depth()\n",
    "        self.tune_child()\n",
    "        self.tune_sampling()\n",
    "        \n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return GradientBoostingClassifier(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rf_tuner(base_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(rf_tuner, self).__init__(X, y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.default_params = {\n",
    "            'random_state':0,\n",
    "            'n_jobs': 4\n",
    "        }\n",
    "        \n",
    "    def tune_loss_criterion(self):\n",
    "        params = {\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "\n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "    def tune_est_num(self):\n",
    "        params = {\n",
    "            'n_estimators': [100, 200, 400, 800]\n",
    "        }\n",
    "\n",
    "        self.fit_and_update_params(params)\n",
    "   \n",
    "    def tune_max_depth(self):\n",
    "        params = {\n",
    "            'max_depth': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "        self.fit_and_update_params(params)\n",
    "        \n",
    "    def tune_child(self):\n",
    "        params = {\n",
    "            'min_samples_split': [2, 3, 7, 15,31],\n",
    "            'min_impurity_decrease': [0, 0.1, 0.2]\n",
    "        }\n",
    "        self.fit_and_update_params(params)\n",
    "     \n",
    "    def tune_sampling(self):\n",
    "        params = {\n",
    "            'max_features': [1., .8, .6, .4, .2]\n",
    "        }\n",
    "        best_parmas = self.fit_and_update_params(params, update=False)\n",
    "        \n",
    "        next_params = {}\n",
    "        for k,v in best_parmas.items():\n",
    "            if v == 1.:\n",
    "                next_params[k] = [1., .95, .9, .85]\n",
    "            else:\n",
    "                next_params[k] = [v+.15, v+.1, v+.05, v, v-.05, v-.1, v-.15]\n",
    " \n",
    "        self.fit_and_update_params(next_params)\n",
    "      \n",
    "    def tune(self):\n",
    "        print('random forest tuner start tuning')\n",
    "        self.tune_loss_criterion()\n",
    "        self.tune_est_num()\n",
    "        self.tune_max_depth()\n",
    "        self.tune_child()\n",
    "        self.tune_sampling()\n",
    "        \n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return RandomForestClassifier(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class et_tuner(rf_tuner):\n",
    "    def __init__(self, X, y):\n",
    "        super(et_tuner, self).__init__(X, y)\n",
    "\n",
    "    def tune(self):\n",
    "        print('extra tree tuner start tuning')\n",
    "        self.tune_loss_criterion()\n",
    "        self.tune_est_num()\n",
    "        self.tune_max_depth()\n",
    "        self.tune_child()\n",
    "        self.tune_sampling()\n",
    "        \n",
    "        return self.get_clf()\n",
    "    \n",
    "    def get_clf(self):\n",
    "        return ExtraTreesClassifier(**self.default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stacking_models_api import StackingAveragedModels\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (8424, 4)\n",
      "Train data X: (1604, 215), y: (1604,)\n",
      "feature count:  215\n",
      "Test data X: (8424, 216)\n",
      "start tuning....\n",
      "\n",
      "logistic regression tuner start tuning\n",
      "Selected hyper-params: {'solver': 'saga'}\n",
      "==============================> cv score: -0.6500\n",
      "lgb tuner start tuning\n",
      "Selected hyper-params: {'learning_rate': 0.05, 'n_estimators': 400}\n",
      "==============================> cv score: -0.5346\n",
      "Selected hyper-params: {'min_split_gain': 0.0, 'num_leaves': 15}\n",
      "==============================> cv score: -0.5098\n",
      "Selected hyper-params: {'colsample_bytree': 0.8, 'subsample': 1.0}\n",
      "==============================> cv score: -0.5082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f7e470cff698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start tuning....\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtuner\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuners\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mclfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-2f551af516f2>\u001b[0m in \u001b[0;36mtune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune_est_num_and_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune_leaves_num_and_gamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune_sampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-2f551af516f2>\u001b[0m in \u001b[0;36mtune_sampling\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mnext_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m.15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_and_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtune_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-f74e4d0e978d>\u001b[0m in \u001b[0;36mfit_and_update_params\u001b[1;34m(self, params, update)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_log_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mcv_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    457\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    197\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1437\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1438\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_df = pd.read_json('Data/test.json')\n",
    "print(\"Test data shape: {}\".format(test_df.shape))\n",
    "test_ids = test_df['id']\n",
    "del test_df\n",
    "gc.enable()\n",
    "gc.collect()\n",
    "\n",
    "train_X, train_y, features = get_train_df()\n",
    "print('feature count: ', len(features))\n",
    "\n",
    "test_X = get_test_df()\n",
    "\n",
    "tuners = [lr_tuner(train_X, train_y),\n",
    "          lgbm_tuner(train_X, train_y), \n",
    "          xgb_tuner(train_X, train_y)]\n",
    "clfs = []\n",
    "print('start tuning....\\n')\n",
    "for tuner in tuners:\n",
    "    tuner.tune()\n",
    "    clfs.append(tuner.get_clf())\n",
    "\n",
    "for i, clf in enumerate(clfs):\n",
    "    if isinstance(tuners[i], lgbm_tuner) or isinstance(tuners[i], xgb_tuner):\n",
    "        if isinstance(tuners[i], lgbm_tuner):\n",
    "            name = 'lgb'\n",
    "        else:\n",
    "            name = 'xgb'\n",
    "\n",
    "        clf.fit(train_X, train_y)\n",
    "        predictions = clf.predict_proba(test_X)[:,1]\n",
    "        print(\"{} tuner predictions\\n\".format(name), predictions)\n",
    "\n",
    "        submission = pd.DataFrame()\n",
    "        submission['id'] = test_ids\n",
    "        submission['is_iceberg'] = predictions\n",
    "        submission.to_csv('Submissions/submission_{}_auto_fine_tune_swwae.csv'.format(name), \n",
    "                          float_format=\"%.15f\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
